{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Evaluating Deep Learning Based Book Recommendation System\n",
    "\n",
    "**Event : Strata Conference , San Francisco, 2019**  \n",
    "\n",
    "In this notebook, we will build and evaluate deep learning based book recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envionrment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utitlity packages\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# data processing and visualization packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# tensorflow packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# ignore warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow version \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating dataset\n",
    "rating_dataset = pd.read_csv(\"data/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore head\n",
    "rating_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of ratings record : \", len(rating_dataset))\n",
    "# number of users and books\n",
    "n_users = len(rating_dataset.user_id.unique())\n",
    "n_items = len(rating_dataset.book_id.unique())\n",
    "print(\"Number of unique users : \", n_users)\n",
    "print(\"Number of unique items : \", n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book metadata \n",
    "book_dataset = pd.read_csv(\"data/books.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(rating_dataset, book_dataset, how='left',left_on='book_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = 53424\n",
    "n_items = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Matrix Factorization Based Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit feedback: supervised ratings prediction\n",
    "\n",
    "For each pair of (user, item) try to predict the rating the user would give to the item.\n",
    "\n",
    "This is the classical setup for building recommender systems from offline data with explicit supervision signal. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Predictive ratings  as a regression problem\n",
    "\n",
    "The following code implements the following architecture:\n",
    "\n",
    "<img src=\"images/01_matrix_factorization.png\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(item_id, user_id, rating):\n",
    "    \"\"\"\n",
    "    parsing each row\n",
    "    \"\"\"\n",
    "    x = {\n",
    "        'User-Input': user_id,\n",
    "        'Item-Input': item_id\n",
    "     }\n",
    "    \n",
    "    y = rating\n",
    "    return x,y    \n",
    "\n",
    "\n",
    "def train_input_fn(csv_path, batch_size=1024, buffer_size=1024):\n",
    "    \"\"\"\n",
    "    train input function \n",
    "    \"\"\"\n",
    "    dataset = (\n",
    "        tf.data.experimental.CsvDataset(\n",
    "            filenames=csv_path,\n",
    "            record_defaults=[tf.int32, tf.int32, tf.int32],\n",
    "            select_cols=[0, 1, 2],\n",
    "            field_delim=\",\",\n",
    "            header=True)\n",
    "        .map(parser)\n",
    "        .shuffle(buffer_size=buffer_size)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(batch_size)\n",
    "    )\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_feats, batch_labels = iterator.get_next()\n",
    "    return batch_feats, batch_labels\n",
    "\n",
    "def eval_input_fn(csv_path, batch_size=1000):\n",
    "    \"\"\"\n",
    "    eval input function\n",
    "    \"\"\"\n",
    "    dataset = (\n",
    "        tf.data.experimental.CsvDataset(\n",
    "            filenames=csv_path,\n",
    "            record_defaults=[tf.int32, tf.int32, tf.int32],\n",
    "            select_cols=[0, 1, 2],\n",
    "            field_delim=\",\",\n",
    "            header=True)\n",
    "        .map(parser)\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_feats, batch_labels = iterator.get_next()\n",
    "    return batch_feats, batch_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Estimator ( Simple Model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(tf_embedding_size, tf_model_dir):\n",
    "    \n",
    "    # creating book embedding path\n",
    "    item_input = Input(shape=[1], name=\"Item-Input\")\n",
    "    item_embedding = Embedding(n_items+1, tf_embedding_size, name=\"Item-Embedding\")(item_input)\n",
    "    item_vec = Flatten(name=\"Flatten-Items\")(item_embedding)\n",
    "\n",
    "    # creating user embedding path\n",
    "    user_input = Input(shape=[1], name=\"User-Input\")\n",
    "    user_embedding = Embedding(n_users+1, tf_embedding_size, name=\"User-Embedding\")(user_input)\n",
    "    user_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\n",
    "\n",
    "    # performing dot product and creating model\n",
    "    prod = Dot(name=\"Dot-Product\", axes=1)([item_vec, user_vec])\n",
    "    model = Model([user_input, item_input], prod)\n",
    "    model.compile('adam', 'mean_squared_error')\n",
    "    model.summary()\n",
    "    return tf.keras.estimator.model_to_estimator(keras_model=model,model_dir=tf_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "tf_model_dir = \"/tmp/model_1/\"\n",
    "tf_data_dir = \"data/ratings.csv\"\n",
    "tf_batch_size = 1024\n",
    "tf_train_steps = 200\n",
    "tf_embedding_size = 10\n",
    "\n",
    "# train and eval spec\n",
    "train_spec = tf.estimator.TrainSpec(input_fn = lambda: train_input_fn(tf_data_dir, batch_size=tf_batch_size, buffer_size=tf_batch_size), max_steps=tf_train_steps)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn = lambda: eval_input_fn(tf_data_dir, batch_size=tf_batch_size) ,steps=1,throttle_secs=1,\n",
    "                                      start_delay_secs=1 )\n",
    "\n",
    "# model \n",
    "estimator = get_estimator(tf_embedding_size, tf_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train and evaluate\")\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "print(\"Training done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Deep recommender model\n",
    "\n",
    "We can use deep learning models with multiple layers ( fully connected and dropout ) for the recommendation system.\n",
    "\n",
    "<img src=\"images/02_deep_recsys.png\" style=\"width: 600px;\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(tf_embedding_size, tf_model_dir):\n",
    "    # creating book embedding path\n",
    "    item_input = Input(shape=[1], name=\"Item-Input\")\n",
    "    item_embedding = Embedding(n_items+1, tf_embedding_size, name=\"Item-Embedding\")(item_input)\n",
    "    item_vec = Flatten(name=\"Flatten-Items\")(item_embedding)\n",
    "\n",
    "    # creating user embedding path\n",
    "    user_input = Input(shape=[1], name=\"User-Input\")\n",
    "    user_embedding = Embedding(n_users+1, tf_embedding_size, name=\"User-Embedding\")(user_input)\n",
    "    user_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\n",
    "\n",
    "    # concatenate features\n",
    "    conc = Concatenate()([item_vec, user_vec])\n",
    "\n",
    "    # add fully-connected-layers\n",
    "    fc1 = Dense(128, activation='relu')(conc)\n",
    "    fc2 = Dense(32, activation='relu')(fc1)\n",
    "    out = Dense(1)(fc2)\n",
    "\n",
    "    # Create model and compile it\n",
    "    model = Model([user_input, item_input], out)\n",
    "    model.compile('adam', 'mean_squared_error')\n",
    "    model.summary()\n",
    "    return tf.keras.estimator.model_to_estimator(keras_model=model,model_dir=tf_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "tf_model_dir = \"/tmp/model_2/\"\n",
    "tf_data_dir = \"data/ratings.csv\"\n",
    "tf_batch_size = 1024\n",
    "tf_train_steps = 200\n",
    "tf_embedding_size = 10\n",
    "\n",
    "# train and eval spec\n",
    "train_spec = tf.estimator.TrainSpec(input_fn = lambda: train_input_fn(tf_data_dir, batch_size=tf_batch_size, buffer_size=tf_batch_size), max_steps=tf_train_steps)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn = lambda: eval_input_fn(tf_data_dir, batch_size=tf_batch_size) ,steps=1,throttle_secs=1,\n",
    "                                      start_delay_secs=1 )\n",
    "\n",
    "# model \n",
    "estimator = get_estimator(tf_embedding_size, tf_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train and evaluate\")\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "print(\"Training done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup feature specification for serving\n",
    "tf_export_dir = '/tmp/export/'\n",
    "feature_spec = {\n",
    "    'User-Input' : tf.FixedLenFeature(shape=[1], dtype=np.float32),\n",
    "    'Item-Input' : tf.FixedLenFeature(shape=[1], dtype=np.float32)\n",
    "}\n",
    "print(\"Export saved model\")\n",
    "serving_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n",
    "export_dir = estimator.export_savedmodel(tf_export_dir, \n",
    "                               serving_input_receiver_fn=serving_fn)\n",
    "\n",
    "print(\"Done exporting the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /tmp/export/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir /tmp/export/* --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = tf.contrib.predictor.from_saved_model(\"/tmp/export/1553619952\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data for prediction\n",
    "\n",
    "# all items\n",
    "item_data = np.array(list(set(dataset.id)))\n",
    "\n",
    "# we need to create user data of the same shape\n",
    "user_to_predict = 1  # User ID \n",
    "user_data = np.array([user_to_predict for i in range(len(item_data))]) # repeat user ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inputs represented by Pandas DataFrame.\n",
    "inputs = pd.DataFrame({\n",
    "    'User-Input': user_data,\n",
    "    'Item-Input': item_data\n",
    "})\n",
    "\n",
    "inputs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input data into serialized Example strings.\n",
    "examples = []\n",
    "for index, row in inputs.iterrows():\n",
    "    feature = {}\n",
    "    for col, value in row.iteritems():\n",
    "        feature[col] = tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "    example = tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature=feature\n",
    "        )\n",
    "    )\n",
    "    examples.append(example.SerializeToString())\n",
    "    \n",
    "predictions = predict_fn({'examples': examples})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_fn({'examples': examples})\n",
    "pred = pred['dense_2'].flatten() # output name\n",
    "print(-np.sort(-pred)[:10])\n",
    "# top 10 items \n",
    "recommended_item_ids = (-pred).argsort()[:10]\n",
    "print(recommended_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Books Rated By User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset.user_id == user_to_predict][[\"original_title\",\"small_image_url\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books Recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dataset[book_dataset['id'].isin(recommended_item_ids)][[\"original_title\",\"small_image_url\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
