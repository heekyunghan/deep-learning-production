
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Strata Conference SF 2019 : Deploying Deep Learning Model using Tensorflow Serving</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab title="Strata Conference SF 2019 : Deploying Deep Learning Model using Tensorflow Serving"
                  environment="web"
                  feedback-link="github.com/mariopce">
    
      <google-codelab-step label="Overview of the tutorial" duration="1">
        <p>In this tutorial, You will learn to use <a href="https://www.tensorflow.org/tfx/guide/serving" target="_blank">Tensorflow Serving</a>.TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. This tutorial will help you to use end-to-end model production using Tensorflow Serving.</p>
<p><strong>What You&#39;ll Learn</strong></p>
<ul>
<li>How to build a simple neural network using Tensorflow and Keras</li>
<li>How to export the model for serving</li>
<li>How to serve the model using Tensorflow Serving</li>
<li>How to make predictions using deployed model</li>
</ul>
<p><strong>What You&#39;ll Need</strong></p>
<ul>
<li>An active <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects" target="_blank">GCP project</a></li>
<li>Access to the Google Cloud Shell, available in the <a href="https://console.cloud.google.com/home/dashboard" target="_blank">Google Cloud Console</a></li>
<li>If you&#39;d prefer to complete the codelab on a local machine, you&#39;ll need to have <a href="https://cloud.google.com/sdk/gcloud/" target="_blank">gcloud</a>, <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/#download-as-part-of-the-google-cloud-sdk" target="_blank">kubectl</a>, and <a href="https://www.docker.com/community-edition" target="_blank">docker</a> installed</li>
</ul>
<aside class="warning"><p>You can create a free GCP account with $300 credit. It would be sufficient for the purpose of this tutorial.</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Getting Started" duration="1">
        <h2>Downloading the Project Files</h2>
<p>The first step is to download a copy of the repository created for the tutorial. This codelab can be completed on a local machine, or through Google Cloud Shell ( Recommended):</p>
<p><a href="http://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/meabhishekkumar/deep-learning-production&page=editor" target="_blank">Download in Google Cloud Shell</a></p>
<p><a href="https://github.com/meabhishekkumar/deep-learning-production/archive/master.zip" target="_blank">Download locally</a></p>
<h2>Enabling Boost Mode (Cloud Shell Only)</h2>
<p>If you are running this codelab out of Cloud Shell, you can enable Boost Mode.It can be enabled through the settings dropdown.</p>
<p><img alt="enable_boost_mode" src="img/543a85501094e43b.png"></p>
<h2>Navigate to Code</h2>
<p>Code for this tutorial is available in the <code>01-basic-tensorflow-serving</code> subfolder inside <code>code</code> folder. Let&#39;s navigate to the folder.</p>
<pre><code>$ cd code/01-basic-tensorflow-serving/
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Training &amp; Exporting the model" duration="0">
        <p>In this tutorial, we are creating a very simple model as the core objective is to learn about Tensorflow serving.</p>
<pre><code>$ python model.py
</code></pre>
<p>Here is the code snippet used to build and train the model.</p>
<pre><code>### dataset : input and output
xs = np.array([-1,0,1,2,3,4], dtype=np.float)
ys = np.array([-3,-1,1,3,5,7], dtype=np.float)

### create simple model 
model = tf.keras.Sequential([tf.keras.layers.Dense(units=1, input_shape=[1])])
model.compile(optimizer=&#39;sgd&#39;,loss=&#39;mean_squared_error&#39;)

### train the model
model.fit(xs, ys, epochs=FLAGS.training_iteration)
print(&#34;trained the model&#34;)

</code></pre>
<p>Here is the code snippet used to export the model.</p>
<pre><code># export the model
with tf.keras.backend.get_session() as sess:
    tf.saved_model.simple_save(
        sess,
        export_path,
        inputs={&#39;input_number&#39;: model.input},
        outputs={t.name:t for t in model.outputs})
</code></pre>
<p>Once you run the script the trained model will be saved in subfloder inside <code>output</code> folder.</p>
<h2>Inspect the model</h2>
<p>You can inspect the model prediction signature using <code>saved_model_cli_show</code> command line utility.</p>
<pre><code>$ saved_model_cli show --dir output/* --all
</code></pre>
<p>if above command is not working for you, you can run the following command to install the utility.</p>
<pre><code>$ pip install tensorflow-serving-api
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Serving Model using TF Serving" duration="0">
        <h2>Pull the Tensorflow Serving Image</h2>
<p>In order to serve the model using Tensorflow Serving, we can directly use available official docker image <code>tensorflow/serving</code>.</p>
<pre><code>$ docker pull tensorflow/serving
</code></pre>
<p>This may take sometime. You can test if the image is downloaded using <code>docker images</code> command.</p>
<h2>Run Tensorflow Server with our model</h2>
<pre><code>$ docker run -it --rm -p 8501:8501  -v &#34;$PWD&#34;/output:/models/test -e MODEL_NAME=test tensorflow/serving

</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Test the Predictions using Curl" duration="0">
        <p>You can use <code>curl</code> to test the prediction directly from command line. In actual scenario, the model would be called from UI or another intermediate layers.</p>
<pre><code>curl -d &#39;{&#34;instances&#34;: [{ &#34;input_number&#34; : [10.0] },{ &#34;input_number&#34; : [20.0] }]}&#39; \
  -X POST http://localhost:8501/v1/models/test:predict
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Optimizing TF Serving" duration="0">
        <p>If Model latency is one of the primary concern for you. You can further optimize Tensorflow server.</p>
<p>You may get following message while running the Tensorflow Server.</p>
<p><code>Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</code></p>
<p>In order to fix this, you can build you own the TensorFlow Serving image. First you need to build a development image using the following command.</p>
<pre><code>docker build -t $USER/tensorflow-serving-devel -f Dockerfile.devel https://github.com/tensorflow/serving.git#:tensorflow_serving/tools/docker
</code></pre>
<p>Then you can build a new serving image with our optimized binary and call it <code>$USER/tensorflow-serving</code></p>
<pre><code>docker build -t $USER/tensorflow-serving \
--build-arg TF_SERVING_BUILD_IMAGE=$USER/tensorflow-serving-devel \ https://github.com/tensorflow/serving.git#:tensorflow_serving/tools/docker
</code></pre>
<p>Now you can use this newly created serving image to run the tensorflow server.</p>
<pre><code>$ docker run -it --rm -p 8501:8501  -v &#34;$PWD&#34;/output:/models/test -e MODEL_NAME=test $USER/tensorflow-serving
</code></pre>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>

</body>
</html>
